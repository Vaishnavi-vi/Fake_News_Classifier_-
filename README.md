# ðŸ“° Fake News Classifier using Simple RNN vs Bidirectional RNN

This project implements a **Fake News Detection** system using Deep Learning. It compares the performance of a **Simple RNN** and a **Bidirectional RNN** on classifying news articles as either **real** or **fake**.

---

## ðŸ§  Project Overview

Fake news spreads rapidly on the internet, making automatic detection essential. In this project, we:

- Built a text classification pipeline.
- Preprocessed and vectorized the dataset.
- Trained and evaluated two models:
  - Simple RNN
  - Bidirectional RNN
- Compared their performance using accuracy, precision, recall, and F1-score.

---

## ðŸ—ƒï¸ Dataset

- **Source**: [Kaggle â€“ Fake and Real News Dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
- **Columns Used**:
  - `text` â€“ Full text of the article
  - `label` â€“ Target label (1 = Fake, 0 = Real)

---

## ðŸ”§ Technologies Used

- **Programming Language**: Python
- **Libraries**:
  - `pandas`, `numpy` â€“ Data processing
  - `matplotlib`, `seaborn` â€“ Visualization
  - `nltk`, `re` â€“ Text preprocessing
  - `tensorflow`, `keras` â€“ Model building and training

---

## âš™ï¸ Project Workflow

### 1. Data Cleaning & Preprocessing
- Lowercased the text
- Removed punctuation, numbers, and stopwords
- Tokenized and padded sequences with word2vec Embedding technique.

### 2. Text Vectorization
Used Word2vec Embedding techiniqe `to convert words to sequences with a fixed length`.

### 3. Model Architectures
- Simple RNN
- Bidirectional RNN
  
---

## ðŸ“Š Performance Comparison

| Metric        | Simple RNN | Bidirectional RNN |
|---------------|------------|-------------------|
| Accuracy      | 87.5%      | 99.9%             |
| Precision     | 83.7%      | 100%              |
| Recall        | 93.8%      | 99%               |
| F1-Score      | 83.84%     | 99%               |

> âœ… **Conclusion**: The **Bidirectional RNN** outperforms the **Simple RNN** in all metrics due to its ability to capture both past and future context in the sequence.



